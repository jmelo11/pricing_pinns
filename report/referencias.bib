% Include here your reference list.

% Use BIBTEX format for reference entries:

@article{black1973,
  issn      = {00223808, 1537534X},
  url       = {http://www.jstor.org/stable/1831029},
  abstract  = {If options are correctly priced in the market, it should not be possible to make sure profits by creating portfolios of long and short positions in options and their underlying stocks. Using this principle, a theoretical valuation formula for options is derived. Since almost all corporate liabilities can be viewed as combinations of options, the formula and the analysis that led to it are also applicable to corporate liabilities such as common stock, corporate bonds, and warrants. In particular, the formula can be used to derive the discount that should be applied to a corporate bond because of the possibility of default.},
  author    = {Fischer Black and Myron Scholes},
  journal   = {Journal of Political Economy},
  number    = {3},
  pages     = {637--654},
  publisher = {The University of Chicago Press},
  title     = {The Pricing of Options and Corporate Liabilities},
  urldate   = {2025-04-23},
  volume    = {81},
  year      = {1973}
}

@article{merton1973,
  issn      = {00058556, 23255323},
  url       = {http://www.jstor.org/stable/3003143},
  abstract  = {The long history of the theory of option pricing began in 1900 when the French mathematician Louis Bachelier deduced an option pricing formula based on the assumption that stock prices follow a Brownian motion with zero drift. Since that time, numerous researchers have contributed to the theory. The present paper begins by deducing a set of restrictions on option pricing formulas from the assumption that investors prefer more to less. These restrictions are necessary conditions for a formula to be consistent with a rational pricing theory. Attention is given to the problems created when dividends are paid on the underlying common stock and when the terms of the option contract can be changed explicitly by a change in exercise price or implicitly by a shift in the investment or capital structure policy of the firm. Since the deduced restrictions are not sufficient to uniquely determine an option pricing formula, additional assumptions are introduced to examine and extend the seminal Black-Scholes theory of option pricing. Explicit formulas for pricing both call and put options as well as for warrants and the new "down-and-out" option are derived. The effects of dividends and call provisions on the warrant price are examined. The possibilities for further extension of the theory to the pricing of corporate liabilities are discussed.},
  author    = {Robert C. Merton},
  journal   = {The Bell Journal of Economics and Management Science},
  number    = {1},
  pages     = {141--183},
  publisher = {[Wiley, RAND Corporation]},
  title     = {Theory of Rational Option Pricing},
  urldate   = {2025-04-23},
  volume    = {4},
  year      = {1973}
}

@inproceedings{Wilmott2010PaulWO,
  title  = {Paul Wilmott on Quantitative Finance},
  author = {Paul Wilmott},
  year   = {2010},
  url    = {https://api.semanticscholar.org/CorpusID:153668090}
}

@misc{wang2023expertsguidetrainingphysicsinformed,
  title         = {An Expert's Guide to Training Physics-informed Neural Networks},
  author        = {Sifan Wang and Shyam Sankaran and Hanwen Wang and Paris Perdikaris},
  year          = {2023},
  eprint        = {2308.08468},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2308.08468}
}

@article{HORNIK1989359,
  title    = {Multilayer feedforward networks are universal approximators},
  journal  = {Neural Networks},
  volume   = {2},
  number   = {5},
  pages    = {359-366},
  year     = {1989},
  issn     = {0893-6080},
  doi      = {https://doi.org/10.1016/0893-6080(89)90020-8},
  url      = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  author   = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
  keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}

@article{RAISSI2019686,
  title    = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  journal  = {Journal of Computational Physics},
  volume   = {378},
  pages    = {686-707},
  year     = {2019},
  issn     = {0021-9991},
  doi      = {https://doi.org/10.1016/j.jcp.2018.10.045},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
  author   = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
  keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge–Kutta methods, Nonlinear dynamics},
  abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
}

@article{cybenko:hal-03753170,
  title       = {{Approximation by superpositions of a sigmoidal function}},
  author      = {Cybenko, G.},
  url         = {https://hal.science/hal-03753170},
  journal     = {{Mathematics of Control, Signals, and Systems}},
  publisher   = {{Springer Verlag}},
  volume      = {2},
  number      = {4},
  pages       = {303-314},
  year        = {1989},
  month       = Dec,
  doi         = {10.1007/BF02551274},
  keywords    = {Neural networks ; Approximation ; Completeness},
  pdf         = {https://hal.science/hal-03753170v1/file/Cybenko1989.pdf},
  hal_id      = {hal-03753170},
  hal_version = {v1}
}

@article{WU2023115671,
  title    = {A comprehensive study of non-adaptive and residual-based adaptive sampling for physics-informed neural networks},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  volume   = {403},
  pages    = {115671},
  year     = {2023},
  issn     = {0045-7825},
  doi      = {https://doi.org/10.1016/j.cma.2022.115671},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782522006260},
  author   = {Chenxi Wu and Min Zhu and Qinyang Tan and Yadhu Kartha and Lu Lu},
  keywords = {Partial differential equations, Physics-informed neural networks, Residual point distribution, Non-adaptive uniform sampling, Uniform sampling with resampling, Residual-based adaptive sampling},
  abstract = {Physics-informed neural networks (PINNs) have shown to be effective tools for solving both forward and inverse problems of partial differential equations (PDEs). PINNs embed the PDEs into the loss of the neural network using automatic differentiation, and this PDE loss is evaluated at a set of scattered spatio-temporal points (called residual points). The location and distribution of these residual points are highly important to the performance of PINNs. However, in the existing studies on PINNs, only a few simple residual point sampling methods have mainly been used. Here, we present a comprehensive study of two categories of sampling for PINNs: non-adaptive uniform sampling and adaptive nonuniform sampling. We consider six uniform sampling methods, including (1) equispaced uniform grid, (2) uniformly random sampling, (3) Latin hypercube sampling, (4) Halton sequence, (5) Hammersley sequence, and (6) Sobol sequence. We also consider a resampling strategy for uniform sampling. To improve the sampling efficiency and the accuracy of PINNs, we propose two new residual-based adaptive sampling methods: residual-based adaptive distribution (RAD) and residual-based adaptive refinement with distribution (RAR-D), which dynamically improve the distribution of residual points based on the PDE residuals during training. Hence, we have considered a total of 10 different sampling methods, including six non-adaptive uniform sampling, uniform sampling with resampling, two proposed adaptive sampling, and an existing adaptive sampling. We extensively tested the performance of these sampling methods for four forward problems and two inverse problems in many setups. Our numerical results presented in this study are summarized from more than 6000 simulations of PINNs. We show that the proposed adaptive sampling methods of RAD and RAR-D significantly improve the accuracy of PINNs with fewer residual points for both forward and inverse problems. The results obtained in this study can also be used as a practical guideline in choosing sampling methods.}
}

@misc{chen2018gradnormgradientnormalizationadaptive,
  title         = {GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks},
  author        = {Zhao Chen and Vijay Badrinarayanan and Chen-Yu Lee and Andrew Rabinovich},
  year          = {2018},
  eprint        = {1711.02257},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1711.02257}
}

@misc{wang2020understandingmitigatinggradientpathologies,
  title         = {Understanding and mitigating gradient pathologies in physics-informed neural networks},
  author        = {Sifan Wang and Yujun Teng and Paris Perdikaris},
  year          = {2020},
  eprint        = {2001.04536},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2001.04536}
}

@inbook{ref1,
  title     = {Deep Learning: History and State-of-the-Arts},
  booktitle = {The Development of Deep Learning Technologies: Research on the Development of Electronic Information Engineering Technology in China},
  year      = {2020},
  publisher = {Springer Singapore},
  address   = {Singapore},
  pages     = {1--11},
  abstract  = {This chapter introduces the history and state-of-the-arts of deep learning technologies. In recent years, deep learning has made an immense impact on both academia and industry. Many academic fields have witnessed deep learning-triggered breakthroughs and a rise in deep learning-related papers. Deep learning has become a buzzword at major academic conferences. Deep learning-related patents account for a sizable share of the AI patent applications. Meanwhile, industrial advancements and new products due to deep learning have greatly changed our manner of working, studying, and living. Widely used in a variety of important fields, deep learning has empowered increasing numbers of applications in information retrieval, information exchange, shopping, healthcare, finance, and industrial manufacturing, which undoubtedly will become increasingly important in our daily lives in the future.},
  isbn      = {978-981-15-4584-9},
  doi       = {10.1007/978-981-15-4584-9_1},
  url       = {https://doi.org/10.1007/978-981-15-4584-9_1}
}

@book{alma99148840908702021,
  author    = {Hull, John},
  title     = {Options, futures, and other derivatives },
  year      = {2018 - 2018},
  address   = {Boston},
  booktitle = {Options, futures, and other derivatives},
  edition   = {Ninth edition, Global edition},
  isbn      = {1292212896},
  keywords  = {Futures ; Stock options ; Derivative securities},
  language  = {eng},
  publisher = {Pearson Education Limited}
}

@book{glasserman2004monte,
  abstract    = {"This book develops the use of Monte Carlo methods in finance, and it also uses simulation as a vehicle for presenting models and ideas from financial engineering. It divides roughly into three parts. The first part develops the fundamentals of Monte Carlo methods, the foundations of derivatives pricing, and the implementation of several of the most important models used in financial engineering. The next part describes techniques for improving simulation accuracy and efficiency. The final third of the book addresses special topics: estimating price sensitivities, valuing American options, and measuring market risk and credit risk in financial portfolios." "The most important prerequisite is familiarity with the mathematical tools used to specify and analyze continuous time models in finance, in particular the key ideas of stochastic calculus. Prior exposure to the basic principles of option pricing is useful but not essential." "The book is aimed at graduate students in financial engineering, researchers in Monte Carlo simulation, and practitioners implementing models in industry."--Jacket.},
  added-at    = {2015-02-16T09:29:29.000+0100},
  address     = {New York},
  author      = {Glasserman, Paul},
  biburl      = {https://www.bibsonomy.org/bibtex/2448dd6ebf136393f61f9e28bf24ab190/knaevelboerrar},
  description = {Amazon.com: Monte Carlo Methods in Financial Engineering (Stochastic Modelling and Applied Probability) (v. 53) (9780387004518): Paul Glasserman: Books},
  interhash   = {e959d3cad8180e836f2a43e245a99207},
  intrahash   = {448dd6ebf136393f61f9e28bf24ab190},
  isbn        = {0387004513 9780387004518 1441918221 9781441918222},
  keywords    = {economics mathematics},
  publisher   = {Springer},
  refid       = {52127560},
  timestamp   = {2015-02-16T09:29:29.000+0100},
  title       = {Monte Carlo methods in financial engineering},
  url         = {http://www.amazon.com/Financial-Engineering-Stochastic-Modelling-Probability/dp/0387004513/ref=pd_sim_b_68?ie=UTF8&refRID=1AN8JXSDGMEV2RPHFC2A},
  year        = 2004
}

@article{blackscholes,
  issn      = {00223808, 1537534X},
  url       = {http://www.jstor.org/stable/1831029},
  abstract  = {If options are correctly priced in the market, it should not be possible to make sure profits by creating portfolios of long and short positions in options and their underlying stocks. Using this principle, a theoretical valuation formula for options is derived. Since almost all corporate liabilities can be viewed as combinations of options, the formula and the analysis that led to it are also applicable to corporate liabilities such as common stock, corporate bonds, and warrants. In particular, the formula can be used to derive the discount that should be applied to a corporate bond because of the possibility of default.},
  author    = {Fischer Black and Myron Scholes},
  journal   = {Journal of Political Economy},
  number    = {3},
  pages     = {637--654},
  publisher = {The University of Chicago Press},
  title     = {The Pricing of Options and Corporate Liabilities},
  urldate   = {2025-04-25},
  volume    = {81},
  year      = {1973}
}

@article{lecun,
  author  = {LeCun, Yann and Bengio, Y. and Hinton, Geoffrey},
  year    = {2015},
  month   = {05},
  pages   = {436-44},
  title   = {Deep Learning},
  volume  = {521},
  journal = {Nature},
  doi     = {10.1038/nature14539}
}

@article{bellman1966dynamic,
  title     = {Dynamic programming},
  author    = {Bellman, Richard},
  journal   = {Science},
  volume    = {153},
  number    = {3731},
  pages     = {34--37},
  year      = {1966},
  publisher = {American Association for the Advancement of Science}
}

@misc{heaton2018deeplearningfinance,
  title         = {Deep Learning in Finance},
  author        = {J. B. Heaton and N. G. Polson and J. H. Witte},
  year          = {2018},
  eprint        = {1602.06561},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1602.06561}
}

@misc{crépey2024cvasensitivitieshedgingrisk,
  title         = {CVA Sensitivities, Hedging and Risk},
  author        = {Stéphane Crépey and Botao Li and Hoang Nguyen and Bouazza Saadeddine},
  year          = {2024},
  eprint        = {2407.18583},
  archiveprefix = {arXiv},
  primaryclass  = {q-fin.CP},
  url           = {https://arxiv.org/abs/2407.18583}
}

@misc{huge2020differentialmachinelearning,
  title         = {Differential Machine Learning},
  author        = {Brian Huge and Antoine Savine},
  year          = {2020},
  eprint        = {2005.02347},
  archiveprefix = {arXiv},
  primaryclass  = {q-fin.CP},
  url           = {https://arxiv.org/abs/2005.02347}
}

@article{longstaffshawrtz,
  author  = {Longstaff, Sam},
  year    = {2001},
  month   = {01},
  pages   = {113-147},
  title   = {Valuing american options by simulation: a simple least squares approach},
  volume  = {14},
  journal = {Review of Finance Studies}
}

@article{Han_2018,
  title     = {Solving high-dimensional partial differential equations using deep learning},
  volume    = {115},
  issn      = {1091-6490},
  url       = {http://dx.doi.org/10.1073/pnas.1718942115},
  doi       = {10.1073/pnas.1718942115},
  number    = {34},
  journal   = {Proceedings of the National Academy of Sciences},
  publisher = {Proceedings of the National Academy of Sciences},
  author    = {Han, Jiequn and Jentzen, Arnulf and E, Weinan},
  year      = {2018},
  month     = aug,
  pages     = {8505–8510}
}

@article{BISCHOF2025117914,
  title    = {Multi-Objective Loss Balancing for Physics-Informed Deep Learning},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  volume   = {439},
  pages    = {117914},
  year     = {2025},
  issn     = {0045-7825},
  doi      = {https://doi.org/10.1016/j.cma.2025.117914},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782525001860},
  author   = {Rafael Bischof and Michael A. Kraus},
  keywords = {Partial differential equations, Scientific machine learning, Helmholtz equation, Burgers equation, PINNacle, Loss balancing, Multi-objective optimisation},
  abstract = {Physics-Informed Neural Networks (PINN) are deep learning algorithms that leverage physical laws by including partial differential equations together with a respective set of boundary and initial conditions as penalty terms in their loss function. In this work, we observe the significant role of correctly weighting the combination of multiple competitive loss functions for training PINNs effectively. To this end, we implement and evaluate different methods aiming at balancing the contributions of multiple terms of the PINN’s loss function and their gradients. After reviewing three existing loss scaling approaches (Learning Rate Annealing, GradNorm and SoftAdapt), we propose a novel self-adaptive loss balancing scheme for PINNs named ReLoBRaLo (Relative Loss Balancing with Random Lookback). We extensively evaluate the performance of the aforementioned balancing schemes by solving both forward as well as inverse problems on three benchmark PDEs for PINNs: Burgers’ equation, Kirchhoff’s plate bending equation, Helmholtz’s equation and over 20 PDEs from the ”PINNacle” collection. The results show that ReLoBRaLo is able to consistently outperform the baseline of existing scaling methods in terms of accuracy while also inducing significantly less computational overhead for a variety of PDE classes.}
}

@misc{cho2023separablephysicsinformedneuralnetworks,
  title         = {Separable Physics-Informed Neural Networks},
  author        = {Junwoo Cho and Seungtae Nam and Hyunmo Yang and Seok-Bae Yun and Youngjoon Hong and Eunbyung Park},
  year          = {2023},
  eprint        = {2306.15969},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2306.15969}
}

@misc{hao2023physicsinformedmachinelearningsurvey,
  title         = {Physics-Informed Machine Learning: A Survey on Problems, Methods and Applications},
  author        = {Zhongkai Hao and Songming Liu and Yichi Zhang and Chengyang Ying and Yao Feng and Hang Su and Jun Zhu},
  year          = {2023},
  eprint        = {2211.08064},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2211.08064}
}

@article{karniadakis2021physics,
  title   = {Physics-informed machine learning},
  author  = {Karniadakis, George Em et al.},
  journal = {Nature Reviews Physics},
  volume  = {3},
  pages   = {422--440},
  year    = {2021}
}

@misc{wang2022respectingcausalityneedtraining,
  title         = {Respecting causality is all you need for training physics-informed neural networks},
  author        = {Sifan Wang and Shyam Sankaran and Paris Perdikaris},
  year          = {2022},
  eprint        = {2203.07404},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2203.07404}
}

@misc{banerjee2024pinnsmedicalimageanalysis,
  title         = {PINNs for Medical Image Analysis: A Survey},
  author        = {Chayan Banerjee and Kien Nguyen and Olivier Salvado and Truyen Tran and Clinton Fookes},
  year          = {2024},
  eprint        = {2408.01026},
  archiveprefix = {arXiv},
  primaryclass  = {eess.IV},
  url           = {https://arxiv.org/abs/2408.01026}
}

@misc{kharazmi2019variationalphysicsinformedneuralnetworks,
  title         = {Variational Physics-Informed Neural Networks For Solving Partial Differential Equations},
  author        = {E. Kharazmi and Z. Zhang and G. E. Karniadakis},
  year          = {2019},
  eprint        = {1912.00873},
  archiveprefix = {arXiv},
  primaryclass  = {cs.NE},
  url           = {https://arxiv.org/abs/1912.00873}
}

@article{Hu_2023,
  title     = {Augmented Physics-Informed Neural Networks (APINNs): A gating network-based soft domain decomposition methodology},
  volume    = {126},
  issn      = {0952-1976},
  url       = {http://dx.doi.org/10.1016/j.engappai.2023.107183},
  doi       = {10.1016/j.engappai.2023.107183},
  journal   = {Engineering Applications of Artificial Intelligence},
  publisher = {Elsevier BV},
  author    = {Hu, Zheyuan and Jagtap, Ameya D. and Karniadakis, George Em and Kawaguchi, Kenji},
  year      = {2023},
  month     = nov,
  pages     = {107183}
}

@article{Hu_2022,
  title     = {When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization?},
  volume    = {44},
  issn      = {1095-7197},
  url       = {http://dx.doi.org/10.1137/21M1447039},
  doi       = {10.1137/21m1447039},
  number    = {5},
  journal   = {SIAM Journal on Scientific Computing},
  publisher = {Society for Industrial & Applied Mathematics (SIAM)},
  author    = {Hu, Zheyuan and Jagtap, Ameya D. and Karniadakis, George Em and Kawaguchi, Kenji},
  year      = {2022},
  month     = sep,
  pages     = {A3158–A3182}
}

@misc{li2021fourierneuraloperatorparametric,
  title         = {Fourier Neural Operator for Parametric Partial Differential Equations},
  author        = {Zongyi Li and Nikola Kovachki and Kamyar Azizzadenesheli and Burigede Liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
  year          = {2021},
  eprint        = {2010.08895},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2010.08895}
}

@article{Lu_2021,
  title     = {Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators},
  volume    = {3},
  issn      = {2522-5839},
  url       = {http://dx.doi.org/10.1038/s42256-021-00302-5},
  doi       = {10.1038/s42256-021-00302-5},
  number    = {3},
  journal   = {Nature Machine Intelligence},
  publisher = {Springer Science and Business Media LLC},
  author    = {Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em},
  year      = {2021},
  month     = mar,
  pages     = {218–229}
}

@article{Bhatnagar_2019,
  title     = {Prediction of aerodynamic flow fields using convolutional neural networks},
  volume    = {64},
  issn      = {1432-0924},
  url       = {http://dx.doi.org/10.1007/s00466-019-01740-0},
  doi       = {10.1007/s00466-019-01740-0},
  number    = {2},
  journal   = {Computational Mechanics},
  publisher = {Springer Science and Business Media LLC},
  author    = {Bhatnagar, Saakaar and Afshar, Yaser and Pan, Shaowu and Duraisamy, Karthik and Kaushik, Shailendra},
  year      = {2019},
  month     = jun,
  pages     = {525–545}
}

@article{SUKUMAR2022114333,
  title    = {Exact imposition of boundary conditions with distance functions in physics-informed deep neural networks},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  volume   = {389},
  pages    = {114333},
  year     = {2022},
  issn     = {0045-7825},
  doi      = {https://doi.org/10.1016/j.cma.2021.114333},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782521006186},
  author   = {N. Sukumar and Ankit Srivastava},
  keywords = {Deep learning, Meshfree method, Distance function, R-function, Transfinite interpolation, Exact geometry},
  abstract = {In this paper, we introduce a new approach based on distance fields to exactly impose boundary conditions in physics-informed deep neural networks. The challenges in satisfying Dirichlet boundary conditions in meshfree and particle methods are well-known. This issue is also pertinent in the development of physics informed neural networks (PINN) for the solution of partial differential equations. We introduce geometry-aware trial functions in artificial neural networks to improve the training in deep learning for partial differential equations. To this end, we use concepts from constructive solid geometry (R-functions) and generalized barycentric coordinates (mean value potential fields) to construct ϕ(x), an approximate distance function to the boundary of a domain in Rd. To exactly impose homogeneous Dirichlet boundary conditions, the trial function is taken as ϕ(x) multiplied by the PINN approximation, and its generalization via transfinite interpolation is used to a priori satisfy inhomogeneous Dirichlet (essential), Neumann (natural), and Robin boundary conditions on complex geometries. In doing so, we eliminate modeling error associated with the satisfaction of boundary conditions in a collocation method and ensure that kinematic admissibility is met pointwise in a Ritz method. With this new ansatz, the training for the neural network is simplified: sole contribution to the loss function is from the residual error at interior collocation points where the governing equation is required to be satisfied. Numerical solutions are computed using strong form collocation and Ritz minimization. To convey the main ideas and to assess the accuracy of the approach, we present numerical solutions for linear and nonlinear boundary-value problems over convex and nonconvex polygonal domains as well as over domains with curved boundaries. Benchmark problems in one dimension for linear elasticity, advection-diffusion, and beam bending; and in two dimensions for the steady-state heat equation, Laplace equation, biharmonic equation (Kirchhoff plate bending), and the nonlinear Eikonal equation are considered. The construction of approximate distance functions using R-functions extends to higher dimensions, and we showcase its use by solving a Poisson problem with homogeneous Dirichlet boundary conditions over the four-dimensional hypercube. The proposed approach consistently outperforms a standard PINN-based collocation method, which underscores the importance of exactly (a priori) satisfying the boundary condition when constructing a loss function in PINN. This study provides a pathway for meshfree analysis to be conducted on the exact geometry without domain discretization.}
}

@article{hestonmodel,
 ISSN = {08939454, 14657368},
 URL = {http://www.jstor.org/stable/2962057},
 abstract = {I use a new technique to derive a closed-form solution for the price of a European call option on an asset with stochastic volatility. The model allows arbitrary correlation between volatility and spot-asset returns. I introduce stochastic interest rates and show how to apply the model to bond options and foreign currency options. Simulations show that correlation between volatility and the spot asset's price is important for explaining return skewness and strike-price biases in the Black-Scholes (1973) model. The solution technique is based on characteristic functions and can be applied to other problems.},
 author = {Steven L. Heston},
 journal = {The Review of Financial Studies},
 number = {2},
 pages = {327--343},
 publisher = {[Oxford University Press, Society for Financial Studies]},
 title = {A Closed-Form Solution for Options with Stochastic Volatility with Applications to Bond and Currency Options},
 urldate = {2025-07-06},
 volume = {6},
 year = {1993}
}

@book{brigo2013interest,
  title={Interest Rate Models Theory and Practice},
  author={Brigo, D. and Mercurio, F.},
  isbn={9783662045534},
  series={Springer Finance},
  url={https://books.google.es/books?id=USvrCAAAQBAJ},
  year={2013},
  publisher={Springer Berlin Heidelberg}
}

@article{hullwhitemodel,
    author = {Hull, John and White, Alan},
    title = {Pricing Interest-Rate-Derivative Securities},
    journal = {The Review of Financial Studies},
    volume = {3},
    number = {4},
    pages = {573-592},
    year = {2015},
    month = {04},
    abstract = {This article shows that the one-state-variable interest-rate models of Vasicek (1977) and Cox, Ingersoll, and Ross (1985b) can be extended so that they are consistent with both the current term structure of interest rates and either the current volatilities of all spot interest rates or the current volatilities of all forward interest rates. The extended Vasicek model is shown to be very tractable analytically. The article compares option prices obtained using the extended Vasicek model with those obtained using a number of other models.},
    issn = {0893-9454},
    doi = {10.1093/rfs/3.4.573},
    url = {https://doi.org/10.1093/rfs/3.4.573},
    eprint = {https://academic.oup.com/rfs/article-pdf/3/4/573/24416170/030573.pdf},
}

@book{björk2004arbitrage,
  title={Arbitrage Theory in Continuous Time},
  author={Bj{\"o}rk, T.},
  isbn={9780191533846},
  series={Oxford Finance Series},
  url={https://books.google.es/books?id=TJgjgJATeVIC},
  year={2004},
  publisher={Oxford University Press, Incorporated}
}

@article{VASICEK1977177,
title = {An equilibrium characterization of the term structure},
journal = {Journal of Financial Economics},
volume = {5},
number = {2},
pages = {177-188},
year = {1977},
issn = {0304-405X},
doi = {https://doi.org/10.1016/0304-405X(77)90016-2},
url = {https://www.sciencedirect.com/science/article/pii/0304405X77900162},
author = {Oldrich Vasicek},
abstract = {The paper derives a general form of the term structure of interest rates. The following assumptions are made: (A.1) The instantaneous (spot) interest rate follows a diffusion process; (A.2) the price of a discount bond depends only on the spot rate over its term; and (A.3) the market is efficient. Under these assumptions, it is shown by means of an arbitrage argument that the expected rate of return on any bond in excess of the spot rate is proportional to its standard deviation. This property is then used to derive a partial differential equation for bond prices. The solution to that equation is given in the form of a stochastic integral representation. An interpretation of the bond pricing formula is provided. The model is illustrated on a specific case.}
}