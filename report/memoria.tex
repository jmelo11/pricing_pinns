%----------
%   WARNING
%----------

% This Guide contains Library recommendations based mainly on APA and IEEE styles, but you must always follow the guidelines of your TFG Tutor and the TFG regulations for your degree.

% THIS TEMPLATE IS BASED ON THE APA STYLE 


%----------
% DOCUMENT SETTINGS
%----------

\documentclass[12pt]{report} % font: 12pt

% margins: 2.5 cm top and bottom; 3 cm left and right
\usepackage[
a4paper,
vmargin=2.5cm,
hmargin=3cm
]{geometry}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows.meta,positioning,calc}


% Paragraph Spacing and Line Spacing: Narrow (6 pt / 1.15 spacing) or Moderate (6 pt / 1.5 spacing)
\renewcommand{\baselinestretch}{1.15}
\parskip=6pt

% Color settings for cover and code listings 
\usepackage[table]{xcolor}
\definecolor{azulUC3M}{RGB}{0,0,102}
\definecolor{gray97}{gray}{.97}
\definecolor{gray75}{gray}{.75}
\definecolor{gray45}{gray}{.45}

% PDF/A -- Important for its inclusion in e-Archive. PDF/A is the optimal format for preservation and for the generation of metadata: http://uc3m.libguides.com/ld.php?content_id=31389625. 

% In the template we include the file OUTPUT.XMPDATA. You can download that file and include the metadata that will be incorporated into the PDF file when you compile the memoria.tex file. Then upload it back to your project. 
\usepackage[a-1b]{pdfx}

% LINKS
\usepackage{hyperref}
\hypersetup{colorlinks=true,
	linkcolor=black, % links to parts of the document (e.g. index) in black
	urlcolor=blue} % links to resources outside the document in blue

% MATH EXPRESSIONS
\usepackage{amsmath,amssymb,amsfonts,amsthm}

% Character encoding
\usepackage{txfonts} 
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% English settings
\usepackage[english]{babel} 
\usepackage[babel, english=american]{csquotes}
\AtBeginEnvironment{quote}{\small}

% Footer settings
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\rfoot{\thepage}
\fancypagestyle{plain}{\pagestyle{fancy}}

% DESIGN OF THE TITLES of the parts of the work (chapters and epigraphs or sub-chapters)
\usepackage{titlesec}
\usepackage{titletoc}
\titleformat{\chapter}[block]
{\large\bfseries\filcenter}
{\thechapter.}
{5pt}
{\MakeUppercase}
{}
\titlespacing{\chapter}{0pt}{0pt}{*3}
\titlecontents{chapter}
[0pt]                                               
{}
{\contentsmargin{0pt}\thecontentslabel.\enspace\uppercase}
{\contentsmargin{0pt}\uppercase}                        
{\titlerule*[.7pc]{.}\contentspage}                 

\titleformat{\section}
{\bfseries}
{\thesection.}
{5pt}
{}
\titlecontents{section}
[5pt]                                               
{}
{\contentsmargin{0pt}\thecontentslabel.\enspace}
{\contentsmargin{0pt}}
{\titlerule*[.7pc]{.}\contentspage}

\titleformat{\subsection}
{\normalsize\bfseries}
{\thesubsection.}
{5pt}
{}
\titlecontents{subsection}
[10pt]                                               
{}
{\contentsmargin{0pt}                          
	\thecontentslabel.\enspace}
{\contentsmargin{0pt}}                        
{\titlerule*[.7pc]{.}\contentspage}  


% Tables and figures settings
\usepackage{multirow} % combine cells 
\usepackage{caption} % customize the title of tables and figures
\usepackage{floatrow} % we use this package and its \ ttabbox and \ ffigbox macros to align the table and figure names according to the defined style.
\usepackage{array} % with this package we can define in the following line a new type of column for tables: custom width and centered content
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\DeclareCaptionFormat{upper}{#1#2\uppercase{#3}\par}
\usepackage{graphicx}
\graphicspath{{imagenes/}} % images fodler

% Table layout for social sciences and humanities
\captionsetup*[table]{
	justification=raggedright,
	labelsep=newline,
	labelfont=small,
	singlelinecheck=false,
	labelfont=bf,
	font=small,
	textfont=it
}

% Figure layout for social sciences and humanities
\captionsetup[figure]{
	%name=Figura,
	singlelinecheck=off,
	labelsep=newline,
	font=small,
	labelfont=bf,
	textfont=it
}
\floatsetup[figure]{
    style=plaintop,
    heightadjust=caption,
    footposition=bottom,
    font=small
}

% Figures and tables footnote layout 
\captionsetup*[floatfoot]{
    footfont={small, up}
}

% FOOTNOTES
\usepackage{chngcntr} % continuous numbering of footnotes
\counterwithout{footnote}{chapter}

% CODE LISTINGS 
% support and styling for listings. More information in  https://es.wikibooks.org/wiki/Manual_de_LaTeX/Listados_de_código/Listados_con_listings
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath,amssymb}   % for math symbols & environments
\usepackage{amsthm}            % for theorem environments

% Theorem‐style definitions:
\theoremstyle{plain}           % default: italic body
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}      % upright body
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}          % upright body, no number
\newtheorem*{remark}{Remark}



% Custom listing
\lstdefinestyle{estilo}{ frame=Ltb,
	framerule=0pt,
	aboveskip=0.5cm,
	framextopmargin=3pt,
	framexbottommargin=3pt,
	framexleftmargin=0.4cm,
	framesep=0pt,
	rulesep=.4pt,
	backgroundcolor=\color{gray97},
	rulesepcolor=\color{black},
	%
	basicstyle=\ttfamily\footnotesize,
	keywordstyle=\bfseries,
	stringstyle=\ttfamily,
	showstringspaces = false,
	commentstyle=\color{gray45},     
	%
	numbers=left,
	numbersep=15pt,
	numberstyle=\tiny,
	numberfirstline = false,
	breaklines=true,
	xleftmargin=\parindent
}

\captionsetup*[lstlisting]{font=small, labelsep=period}
 
\lstset{style=estilo}
\renewcommand{\lstlistingname}{\uppercase{Código}}


% REFERENCES 

% APA bibliography setup
%\usepackage[style=apa, backend=biber, natbib=true, hyperref=true, uniquelist=false, sortcites]{biblatex}

\usepackage[style=ieee, backend=biber, natbib=true, hyperref=true, uniquelist=false, sortcites]{biblatex}


\addbibresource{referencias.bib} % The references.bib file in which the bibliography used should be


%-------------
%	DOCUMENT
%-------------

\begin{document}
\pagenumbering{roman} % Roman numerals are used in the numbering of the pages preceding the body of the work.
	
%----------
%	COVER
%----------	
\begin{titlepage}
	\begin{sffamily}
	\color{azulUC3M}
	\begin{center}
		\begin{figure}[H] % UC3M Logo
			\makebox[\textwidth][c]{\includegraphics[width=16cm]{logo_UC3M.png}}
		\end{figure}
		\vspace{2.5cm}
		\begin{Large}
			Master Degree in Computational and Applied Mathematics\\			
			 2024-2025\\ % Academic year
			\vspace{2cm}		
			\textsl{Master Thesis}
			\bigskip
			
		\end{Large}
		 	{\Huge Leveraging Physics-Informed Neural Networks for Option Pricing Problems}\\
		 	\vspace*{0.5cm}
	 		\rule{10.5cm}{0.1mm}\\
			\vspace*{0.9cm}
			{\LARGE Jose Pedro Melo Olivares}\\ 
			\vspace*{1cm}
		\begin{Large}
			Pedro Echeverria, Ph.D \\
			Francisco Bernal, Ph.D\\
			Madrid, 2025\\
		\end{Large}
	\end{center}
	\vfill
	\color{black}
	\fbox{
	\begin{minipage}{\linewidth}
    	\textbf{AVOID PLAGIARISM}\\
    	\footnotesize{The University uses the \textbf{Turnitin Feedback Studio} for the delivery of student work. This program compares the originality of the work delivered by each student with millions of electronic resources and detects those parts of the text that are copied and pasted. Plagiarizing in a TFM is considered a  \textbf{\underline{Serious Misconduct}}, and may result in permanent expulsion from the University.}\end{minipage}}

	% IF OUR WORK IS TO BE PUBLISHED UNDER A CREATIVE COMMONS LICENSE, INCLUDE THESE LINES. IS THE RECOMMENDED OPTION.
	\noindent\includegraphics[width=4.2cm]{creativecommons.png}\\ % Creative Commons Logo
    \footnotesize{This work is licensed under Creative Commons \textbf{Attribution - Non Commercial - Non Derivatives}}
	
	\end{sffamily}
\end{titlepage}

\newpage % blank page
\thispagestyle{empty}
\mbox{}

%----------
%	ABSTRACT AND KEYWORDS 
%----------	
\renewcommand\abstractname{\large\bfseries\filcenter\uppercase{Summary}}
\begin{abstract}
\thispagestyle{plain}
\setcounter{page}{3}
	
	% Write your abstract  

This master's thesis explores the usage of physics-informed neural networks (PINNs) in 
option pricing problems, as they allow for improved evaluation speeds compared to traditional methods. This 
document presents results that confirm that PINNs are capable of tackling this problem with 
satisfactory results.
	
	\textbf{Keywords:} % add the keywords
	
	\vfill
\end{abstract}
	\newpage % Blank page
	\thispagestyle{empty}
	\mbox{}


%----------
%	Dedication
%----------	
\chapter*{Dedication}

\setcounter{page}{5}
	
	% Write here	
    To my partner, Paula, for her support, to Francisco Gomez and Pedro Echeverria from BBVA for their valuable feedback and guidance and to the UC3M math faculty for their help.
	\vfill
	
	\newpage % blank page
	\thispagestyle{empty}
	\mbox{}
	

%----------
%	TOC
%----------	

%--
% TOC
%-
\tableofcontents
\thispagestyle{fancy}

\newpage % blank page
\thispagestyle{empty}
\mbox{}

%--
% List of figures. If they are not included, comment the following lines
%-
\listoffigures
\thispagestyle{fancy}

\newpage % blank page
\thispagestyle{empty}
\mbox{}

%--
% List of tables. If they are not included, comment the following lines
%-
\listoftables
\thispagestyle{fancy}

\newpage % blankpage
\thispagestyle{empty}
\mbox{}


%----------
%	THESIS
%----------	
\clearpage
\pagenumbering{arabic} % numbering with Arabic numerals for the rest of the document.	

\chapter{Introduction}

\section{Motivation}

The fast advances in machine learning over the past decade have profoundly transformed 
fields such as speech recognition, visual object recognition, object detection and many other domains such 
as drug discovery and genomics \cite{lecun}. Among these fields, the financial industry stands out by continuously 
seeking efficient and accurate computational methods for pricing complex financial products. Derivatives, 
which are financial instruments whose value depends on underlying assets \cite{alma99148840908702021}, are particularly 
significant due to their extensive use for risk management, speculation, and hedging  
strategies.

Derivative's underlyings can be a wide range of financial assets, such
as stocks, bonds, commodities, or indices \cite{Wilmott2010PaulWO}. The accurate pricing of such products is crucial in 
finance, as it influences investment decisions and risk management strategies and traditionally it has been done using
either Monte Carlo simulations \cite{glasserman2004monte} or solving Partial Differential 
Equations (PDEs) -like the renowned Black-Scholes equation \cite{blackscholes}. While PDE-based methods provide 
valuable theoretical insights, their applicability has been restricted by the "curse of 
dimensionality" \cite{bellman1966dynamic} and the speed of evaluation, particularly when dealing with derivatives dependent on 
multiple assets or path-dependent features. As a result, Monte Carlo simulations have historically been the 
preferred numerical approach in practice. However, the advent of different 
machine-learning-based techniques now presents an opportunity to overcome these constraints effectively \cite{Han_2018}.

The emergence of Physics-Informed Neural Networks (PINNs) \cite{RAISSI2019686}, a modern machine learning 
methodology, provides an innovative approach that addresses these challenges. PINNs leverage 
neural networks not only as universal approximators of functions but also explicitly 
incorporate the governing PDEs \cite{RAISSI2019686} of the underlying financial model into their training. This 
integration enables PINNs to learn directly from the mathematical structure of the problem, 
ensuring accuracy and consistency with the financial theory while dramatically reducing 
computation times, at the expense of amortized training times. This advantage is particularly appealing 
to financial institutions that require real-time or near-real-time pricing for complex derivatives.

Additionally, PINNs can generate complete price surfaces fast, facilitating immediate 
sensitivity analyses and risk assessment across a broad range of market scenarios. Such 
capabilities are particularly valuable in context like valuation adjustments -also know as XVA-, and internal modeling frameworks.

This master's thesis aims to explore and validate the effectiveness of PINNs in solving 
derivative pricing problems. Guided by industry professionals from BBVA, 
the research presented herein assesses the capability of PINNs to efficiently and accurately 
price complex derivatives through PDEs, demonstrating significant potential 
for practical adoption in financial institutions.

The subsequent chapters will present a comprehensive review of derivative pricing theory, 
introduce the mathematical foundations and computational framework of PINNs, describe the 
specific implementation details, and thoroughly evaluate their performance against traditional pricing methods.

\section{Objectives}

The primary objective of this thesis is to investigate the use of PINNs for the pricing of financial 
derivatives involving different dynamics and products and explore the potential this technique
as a competitive alternative to traditional numerical procedure such as Monte Carlo simulations and finite difference methods.

To this end, the specific objectives are as follows:
\begin{itemize}
    \item To review the theoretical foundations of derivative pricing and the associated PDEs for some representative selection of derivative products.
    \item To provide a concise overview of classical numerical methods for solving PDEs in the context of derivative pricing and highlighting their limitations.
    \item To assess the overall effectiveness and scalability of PINNs in pricing both single-asset and multi-asset derivatives.
    % \item To introduce the concept of PINNs and explain their mathematical and computational foundations, including network architecture and loss function formulation.
    % \item To design and implement various experimental setups using PINNs, evaluating the effect of different network shapes, loss balancing techniques, sampling strategies, and solver configurations.
    % \item To compare the performance of different quasi-Newton solvers and optimization strategies within the PINNs framework.
\end{itemize}

This thesis also aims to provide practical insights relevant to the financial industry, supported by 
guidance from professionals at BBVA, and to contribute to the growing body of research that bridges 
deep learning and quantitative finance.

\section{Thesis Structure}
This thesis is structured into six main chapters, each addressing a different aspect of the research:
\begin{itemize}
    \item \textbf{Chapter 1: Introduction} \\
    Presents the motivation behind the study, defines the objectives, and outlines the structure of the 
	thesis.
    
    \item \textbf{Chapter 2: Theoretical Background} \\
    Introduces the fundamental concepts of derivative pricing, including the Black-Scholes model and 
	its multi-asset extension. It also provides a brief overview of traditional numerical methods, such 
	as finite difference methods and Monte Carlo simulations, and discusses their limitations in 
	high-dimensional settings.
    
    \item \textbf{Chapter 3: Physics-Informed Neural Networks} \\
    Explains the concept of PINNs, starting with the theoretical underpinnings of neural networks. It 
	then discusses the structure of PINNs, their components, and the training methodology used to enforce 
	the underlying PDE constraints.
    
    % \item \textbf{Chapter 4: Implementation and Experiments} \\
    % Details the experimental setup, including the architecture of the neural networks, optimization 
	% strategies, and various techniques explored to improve model performance. This includes comparisons 
	% of solver strategies, network configurations, loss balancing, and data sampling techniques.
    
    \item \textbf{Chapter 4: Implementation and Results} \\
    Presents the results obtained from both single-asset and multi-asset derivative pricing experiments. 
	The performance of the PINNs is analyzed and compared to expectations, highlighting strengths, 
	challenges, and insights gained from the implementation.
    
    \item \textbf{Chapter 5: Conclusions and Future Work} \\
    Summarizes the main findings of the study, reflects on the limitations encountered, and proposes 
	directions for future research in the application of PINNs in quantitative finance.
\end{itemize}

This structure is intended to provide a logical and progressive narrative that moves from theory to 
implementation and finally to critical evaluation and discussion.

\chapter{Theoretical Background}
This chapter provides the foundational concepts necessary for understanding the pricing of financial 
derivatives, the mathematical tools traditionally used for their valuation, and the challenges 
associated with high-dimensional problems. It begins by introducing what derivatives are and the 
different types commonly traded in financial markets, before discussing the theoretical models used 
to price them and the limitations of classical numerical methods.

\section{Derivative Pricing Fundamentals}

\subsection{Derivatives Overview}\label{sec:derivatives_overview}

Financial derivatives are contracts whose cash flows are linked to the value of one or several
\emph{underlyings}—shares, commodities, foreign-exchange rates, or interest-rate curves
\cite{alma99148840908702021,Wilmott2010PaulWO}.  
By re-packaging the risk of these underlyings, derivatives let market participants
insure portfolios, express directional views with limited capital, and design bespoke payoffs
that would be hard to replicate through spot trading alone.

\medskip
\noindent\textbf{Single-asset vanilla options.}
The starting point for almost every option textbook is the \emph{European} call or put.
A European call delivers, at a fixed future date \(T\), the right to buy the underlying at the
pre-agreed strike \(K\); the put grants the right to sell. The payoffs
\[
\max(S_T-K,0),\qquad \max(K-S_T,0)
\]
are therefore functions of the future spot price \(S_T\) only, so valuing the contract amounts
to forecasting the \emph{distribution} of \(S_T\) under a risk-neutral measure. The celebrated
Black-Scholes formula achieves this under a log-normal assumption for returns, but observed
market prices exhibit persistent \emph{volatility smiles}, forcing practitioners to enrich the
model with time-varying or random volatility. Such extensions break the closed-form solution
and motivate numerical solvers—finite differences, Fourier integrals, Monte-Carlo simulations,
and, as this thesis explores, Physics-Informed Neural Networks (PINNs).

\medskip
\noindent\textbf{Early-exercise complexity: the American put.}
An American option can be exercised \emph{any time} up to expiry. Because the holder may choose
the most favourable moment, the pricing problem now involves deciding when to stop as well as
what the discounted payoff might be.  In mathematical terms the valuation becomes a
\emph{free-boundary} problem: one must find both the option price and the moving boundary
separating the \emph{hold and exercise} regions. Tree methods, finite-difference schemes, and
regression-based Monte-Carlo algorithms cope with this added layer, but each suffers either
slow convergence or high variance.

\medskip
\noindent\textbf{Going multi-asset: best-of basket calls.}
Retail structured notes often promise the performance of “whichever of these three tech stocks
does best.”  Mathematically the payoff is
\[
\max\bigl(\max_{1\le i\le d}S_{i,T}-K,0\bigr),
\]
where only the strongest performer among \(d\) assets matters.  Pricing therefore depends on
how the assets move \emph{together}.  
Even if each asset were independent, traditional finite-difference grids would expand
exponentially with \(d\); when cross-correlation is introduced, the curse of dimensionality
worsens. Monte-Carlo sampling remains feasible but noisy. These basket options hence provide an ideal test bed
for high-dimensional PINN techniques.

\medskip
\noindent\textbf{Adding market factors: equity calls with stochastic volatility and rates.}
Long-dated equity or FX options are not only sensitive to the level of the underlying
but also to the path followed by volatility and by interest rates used to discount future
cash flows. In industry a popular approach is to combine a
stochastic-volatility model (capturing the volatility smile) with a one-factor
description of the short-term interest rate (capturing the term-structure of discount
curves).  Although each ingredient is tractable in isolation, their combination produces a
three-dimensional valuation problem with no analytic shortcut.

\medskip
\noindent\textbf{Interest-rate derivatives.}
Unlike equity options, which depend on a traded asset, interest-rate contracts are tied
to an entire curve of future borrowing costs.  A \emph{payer swaption} gives its owner the
right, at time \(T\), to lock in a fixed rate \(K\) on a standard interest-rate swap running
from \(T\) to some final date \(T_n\).  If, at expiry, the prevailing market swap rate
\(L(T)\) exceeds \(K\), the option is in the money and the holder enters the swap; otherwise it
is left to lapse.  One can think of the payoff as the present value of a series of future
fixed-vs-floating exchanges, multiplied by \(\max(L(T)-K,0)\).

To compute today's price, a model must therefore (i) fit the current discount curve so that the
valuation of the floating leg is consistent with observable bond prices, and (ii) describe how
the whole curve might move between now and \(T\).  The one-factor Hull-White model remains a
workhorse because it achieves task (i) exactly while keeping task (ii) low-dimensional.
However, even with this simplification closed-form solutions exist only for the cleanest
European style; callability clauses, collateral requirements, or cash-settlement conventions
re-introduce numerical integration.  PINNs offer a flexible alternative: once the governing
partial differential equation is embedded into the neural network, curve fitting and option
valuation are solved simultaneously.

\medskip
\noindent\textbf{Connecting the dots.}
From the plain European call to the swaption, each contract adds a new layer of
reality—early-exercise privilege, multiple underlyings, extra risk factors, or dependence on an
entire yield curve.  These layers defeat the neat closed forms of classical option theory and
stretch grid-based solvers beyond practicality.  PINNs, by contrast, are mesh-free,
dimension-agnostic, and naturally incorporate boundary or inequality constraints in their loss
functions.  The remainder of the thesis will quantify how these attributes translate into
speed and accuracy across our five representative derivatives
\cite{huge2020differentialmachinelearning,heaton2018deeplearningfinance}.

\section{Modelling Framework}\label{sec:modelling_framework}

Financial derivatives are priced using tools derived from stochastic calculus.
Throughout this section we fix a filtered probability space
$(\Omega,\mathcal{F},(\mathcal{F}_t)_{t\ge0},\mathbb{Q})$ satisfying
the usual conditions and interpret $\mathbb{Q}$ as a
\emph{risk-neutral} measure.  All stochastic processes introduced
below are $(\mathcal{F}_t)$-adapted and, unless stated otherwise,
driven by one or several standard $\mathbb{Q}$-Brownian motions with
the specified instantaneous correlation.

%---------------------------------------------------------------------
\subsection{The Black-Scholes Model}\label{sec:bs_dynamics}

We begin by explaining option price under the assumptions of the Black-Scholes model. This framework, proposed by \cite{black1973},
assumes option prices are a consecuence of \textit{hedging}, a procedure used to eliminate risk 
associated with the fluctuation of the underlying asset. The argument used is as follows.

Let's assume that a price process behaves as a random variable $(S_t)_{t\ge0}$ that follows a geometric 
Brownian motion give by the stochastic differential process:
\begin{equation}\label{eq:gbm_single_redux}
  \mathrm{d}S_t
  = \mu\,S_t\,\mathrm{d}t
  + \sigma\,S_t\,\mathrm{d}W^{\mathbb{P}}_t,
  \qquad
  S_0>0,
\end{equation}
where $\mu$ is the mean of the process and $\sigma$ its annualized volatility. Now let us propose a 
portfolio composed of a sum of the derivative $V(S, t) := V_t$, and some units of the underlying asset.
This portfolio is also a stochastic process given by
\begin{equation}
  \Pi_t = V_t- \Delta S_t.
\end{equation}
A well know fact from economics \cite{} is that riskless porfolios are required to yield the same rate of return
as risk-free investments. This implies that $\Pi_t$ evolves as
\begin{equation}\label{portfolio}
	\Pi_t = e^{rt}\implies \mathrm{d}\Pi_t = r\Pi_t\mathrm{d}t = r(V_t-\Delta S_t)\mathrm{d}t.
\end{equation}
Applying Ito's lemma to the left side we get
\begin{equation}\label{riskfree_portfolio}
	\mathrm{d}\Pi_t = \mathrm{d}V_t - \Delta S_t,
\end{equation}
where $\mathrm{d}V$ is given by the following PDE
\begin{equation}\label{ito_vadjust}
	\mathrm{d}V_t = \Big(\frac{\partial V}{\partial t}+\mu S \frac{\partial V}{\partial t} + 
	\sigma^2 S^2\frac{\partial^2 V}{\partial S^2}\Big)\mathrm{d}t + \sigma \frac{\partial V}{\partial S} \mathrm{d}W^{\mathbb{P}}_t.
\end{equation}
Choosing  
\[
\Delta_t \;=\; \frac{\partial V}{\partial S}(t,S_t)
\]
eliminates the Brownian increment in the hedged portfolio, 
because the $\sigma\,\partial_S V\,\mathrm{d}W^{\mathbb{P}}_t$ term in
$\mathrm{d}V_t$ exactly cancels the corresponding term in
$\Delta_t\,\mathrm{d}S_t$.  The remaining drift of $\Pi_t$
must equal $r\Pi_t\,\mathrm{d}t$ by \eqref{portfolio}, so
\[
r\bigl(V_t-\Delta_t S_t\bigr)
\;=\;
\frac{\partial V}{\partial t}
\;+\;
\mu S_t\frac{\partial V}{\partial S}
\;+\;
\frac{1}{2}\sigma^{2}S_t^{2}\frac{\partial^{2}V}{\partial S^{2}}.
\]
Substituting $\Delta_t$ and rearranging yields the
\emph{Black--Scholes partial differential equation}
\begin{equation}
\boxed{
\frac{\partial V}{\partial t}
\;+\;
\frac{1}{2}\sigma^{2}S^{2}\frac{\partial^{2}V}{\partial S^{2}}
\;+\;
rS\,\frac{\partial V}{\partial S}
\;-\;
rV
\;=\;0,
}\label{eq:BS_complete}
\end{equation}
to be solved for $V(t,S)$ with terminal condition
$V(T,S)=\Phi(S)$, where $\Phi$ denotes the contract’s payoff. For the sake of notation,
we introduce the operator $\mathcal{L}$, which is defined for the Black-Scholes model as
\begin{equation}\label{eq:L_BS}
\mathcal{L}^{\mathrm{BS}}
:= \frac{1}{2}\sigma^{2}S^{2}\frac{\partial^{2}}{\partial S^{2}}
	+ rS\,\frac{\partial }{\partial S},
\end{equation}
and rewrite \eqref{eq:BS_complete} as
\begin{equation}\label{eq:BS_complete_redux}
\frac{\partial V}{\partial t}+\mathcal{L}^{\mathrm{BS}}V-rV=0.
\end{equation}

One key result of the Black-Scholes model is that the risk-neutral dynamics of the underlying asset
must has a drift equal to the risk-free rate $r$ in the risk-neutral measure $\mathbb{Q}$. This fact is implied by the
Feyman-Kac theorem, which states that the solution to the PDE \eqref{eq:BS_complete_redux} can be expressed as
\begin{equation}\label{eq:FK_solution}
V(t,S) = e^{-r(T-t)}\mathbb{E}^{\mathbb{Q}}[\Phi(S_T)|S_t=S],
\end{equation}
where $\mathbb{E}^{\mathbb{Q}}[\cdot]$ denotes the expectation under the risk-neutral measure $\mathbb{Q}$ and $S_t$ follows the
geometric Brownian motion \eqref{eq:gbm_single_redux} with drift $\mu=r$.


\subsection{Multi-Asset Extension of the Black-Scholes Model}\label{sec:multiasset}

Now we proceed to extend the model for multiple underlying assets. From the previous section we know that the underlying assets
must have a drift equal to the risk-free rate $r$ in the risk-neutral measure $\mathbb{Q}$. For $d\ge2$ underlyings the vector
$\mathbf{S}_t=(S^1_t,\dots,S^d_t)^{\!\top}$ satisfies
\begin{equation}\label{eq:gbm_multi_redux}
  \mathrm{d}S^i_t
  = r\,S^i_t\,\mathrm{d}t
  + \sigma_i S^i_t\,\mathrm{d}W^{\mathbb{Q},i}_t,
  \qquad
  \mathrm{d}W^{\mathbb{Q},i}_t\,\mathrm{d}W^{\mathbb{Q},j}_t
  = \rho_{ij}\,\mathrm{d}t,
  \quad 1\le i,j\le d.
\end{equation}
Let $V(t,\mathbf{S})$ denote the option value. The same replication argument gives
\begin{equation}\label{eq:L_multi}
  \mathcal{L}^{\mathrm{BS},d}
  := \frac12\sum_{i=1}^d\sum_{j=1}^{d}
        \sigma_i\sigma_j\rho_{ij}
        S_iS_j\,\frac{\partial }{\partial S_i S_j}
     + r\sum_{i=1}^{d}S_i\partial_{S_i}
     - r,
\end{equation}
with the same PDE to solve 
\begin{equation}
	\frac{\partial V}{\partial t} + \mathcal{L}^{\mathrm{BS},d}V - rV=0.
\end{equation}

As the goal is to price using PINNs, is important to mention this technique benefits 
from input variables that are centred and have comparable magnitudes.  
Its possible to achieve this by expressing prices in \emph{log-moneyness}
$x_i:=\ln(S_i/K)$ and by running the problem backward in time.
Formally, for maturity \(T\) define
\[
x_i=\ln\frac{S_i}{K},\qquad
\tau=T-t,\qquad
u(\tau,\mathbf{x})=\frac{V(t,\mathbf{S})}{K},
\]
where \(\mathbf{S}=(S_1,\dots,S_d)^{\!\top}\) and
\(K\) is the strike. Starting from the multi-asset Black-Scholes PDE and applying 
the above change of variables, the chain rule yields the
 \emph{dimensionless backward} equation  
\begin{equation}
  \frac{\partial u}{\partial \tau} =
    \frac12\sum_{i=1}^{d}\sigma_i^{2}
        \Bigl(
            \frac{\partial^{2}u}{\partial x_i^{2}}
          - \frac{\partial u}{\partial x_i}
        \Bigr)
  + \frac12\sum_{i,j=1}^{d}\sigma_i\sigma_j\rho_{ij}
        \,\frac{\partial^{2}u}{\partial x_i\,\partial x_j}
  + r\sum_{i=1}^{d}\frac{\partial u}{\partial x_i}
  - ru.
  \label{eq:scaled_PDE_exact}
\end{equation}
to be solved for
\(\tau\in(0,T]\) and \(\mathbf{x}\in\mathbb{R}^{d}\) with terminal
condition \(u(0,\mathbf{x})=\Phi(\mathbf{x})/K\). Again, it is convenient to rewrite \eqref{eq:scaled_PDE_exact} in operator
form
\[
\;
-\frac{\partial u}{\partial \tau} + 
\widetilde{\mathcal{L}}^{\mathrm{BS},d}u - ru = 0,
\qquad
u(0,\mathbf{x})=\frac{\Phi(\mathbf{x})}{K}
\;,
\]
where the \emph{dimensionless generator}
\begin{equation}\label{eq:L_tilde_BS}
  \;
  \widetilde{\mathcal{L}}^{\mathrm{BS},d}
  := \frac{1}{2}\sum_{i=1}^{d}\sum_{j=1}^{d}
       \sigma_i\sigma_j\rho_{ij}\,
       \frac{\partial}{\partial x_i x_j}
     - \frac{1}{2}\sum_{i=1}^{d}\sigma_i^{2}\,
       \frac{\partial}{\partial x_i}
  \;
\end{equation}
acts on twice differentiable test functions
and is elliptic with constant coefficients.
Equation~\eqref{eq:scaled_PDE_final} will serve as the reference
problem for the mesh-free PINN solver in
Section~\ref{sec:pinn_implementation}.

%---------------------------------------------------------------------
\subsection{Extensions Beyond Black-Scholes}\label{sec:extensions}

\subsubsection{Stochastic Volatility}
Its a common observation in the market that option prices do not follow the
log-normal distribution assumed by the Black-Scholes model. In general, the implied volatilities vary with strike and
maturity, forming the well-known \emph{volatility smile}. A single constant parameter~$\sigma$ is
therefore insufficient to reproduce observed prices \cite{Wilmott2010PaulWO}.  
Moreover, for maturities beyond one year the assumption of a
deterministic discount rate becomes untenable; the entire yield curve
moves randomly in response to macroeconomic news and monetary policy
\cite{Wilmott2010PaulWO}. This is key when pricing long-dated
equity options, which are sensitive to equity, volatility, and interest rate dynamics.  These
considerations motivate models that combine \emph{stochastic
volatility} with \emph{stochastic interest rates}.

Among the many specifications proposed, the square-root process of
\cite{hestonmodel} offers a parsimonious and analytically tractable
alternative to Black-Scholes.  Under the risk-neutral measure
$\mathbb{Q}$ the state vector $(S_t,v_t)$ satisfies
\begin{equation}\label{eq:heston_sde}
\begin{aligned}
  \mathrm{d}S_t &= r\,S_t\,\mathrm{d}t
                + \sqrt{v_t}\,S_t\,\mathrm{d}W_t^{\mathbb{Q},S},\\
  \mathrm{d}v_t &= \kappa(\theta-v_t)\,\mathrm{d}t
                + \sigma_v\sqrt{v_t}\,\mathrm{d}W_t^{\mathbb{Q},v},\\
  \mathrm{d}W_t^{\mathbb{Q},S}\,\mathrm{d}W_t^{\mathbb{Q},v}
                &=\rho\,\mathrm{d}t,
\end{aligned}
\end{equation}
where $\kappa>0$ is the mean-reversion speed,
$\theta>0$ the long-run variance,
$\sigma_v>0$ the volatility of volatility, and
$\rho\in[-1,1]$ the instantaneous correlation.
The Feller condition $2\kappa\theta\!>\!\sigma_v^{2}$
guarantees $v_t>0$ almost surely.

In order to get the pricing PDE, we set $V(t,S,v)$ to denote the price, at time $t$, of a derivative maturing
at $T$ with payoff $\Phi(S_T)$. Assuming $V$ is twice continuously
differentiable in $S$ and $v$ and once in $t$, Itô's lemma applied to
$V(t, S_t, v_t)$ gives
\[
\begin{aligned}
\mathrm{d}V
&=
\Biggl(
      \frac{\partial V}{\partial t}
    + r S \frac{\partial V}{\partial S}
    + \kappa(\theta-v)\,\frac{\partial V}{\partial v}
    + \frac{1}{2} v S^{2}\,\frac{\partial^{2} V}{\partial S^{2}}
    + \frac{1}{2}\sigma_{v}^{2} v\,\frac{\partial^{2} V}{\partial v^{2}}
    + \rho\,\sigma_{v} v S\,\frac{\partial^{2} V}{\partial S\,\partial v}
\Biggr)\,\mathrm{d}t \\[6pt]
&\quad
  + \sqrt{v}\,S\,\frac{\partial V}{\partial S}\,
      \mathrm{d}W_{t}^{\mathbb{Q},S}
  + \sigma_{v}\sqrt{v}\,\frac{\partial V}{\partial v}\,
      \mathrm{d}W_{t}^{\mathbb{Q},v}.
\end{aligned}
\]
From the risk-neutral valuation theory \cite{björk2004arbitrage}, we know that all assets, including the derivative
$V$, must have a drift equalt to the risk-free rate $r$ in the risk-neutral measure $\mathbb{Q}$.
This implies that the drift of the process $\mathrm{d}V$ must be equal to
\begin{equation}
\mathrm{d}V_t = r V_t \mathrm{d}t.
\end{equation}
Using this fact and the Feynman-Kac theorem, which states that given the terminal condition $V(T,S,v)=\Phi(S)$, the equation
\begin{equation}\label{eq:FK_heston}
V(t,S,v) = e^{-r(T-t)}\mathbb{E}^{\mathbb{Q}}[\Phi(S_T)|S_t=S,v_t=v],
\end{equation}
is solution to the PDE
\begin{equation}\label{eq:heston_pde}
\begin{aligned}
  &\frac{\partial V}{\partial t}
  + \tfrac12 v S^{2}\,\frac{\partial^{2} V}{\partial S^{2}}
  + \rho\sigma_{v} v S\,\frac{\partial^{2} V}{\partial S\,\partial v}
  + \tfrac12\sigma_{v}^{2} v\,\frac{\partial^{2} V}{\partial v^{2}}
  + \kappa(\theta-v)\,\frac{\partial V}{\partial v}
  + r S\,\frac{\partial V}{\partial S}
  - r V
  = 0,
\end{aligned}
\end{equation}
which is the \emph{Heston PDE}. Again, we can rewrite this PDE in operator form
\begin{equation}\label{eq:L_Heston}
  \mathcal{L}^{\mathrm{Heston}}
  := \frac{1}{2}v S^{2}\frac{\partial^{2}}{\partial S^{2}}
  + \rho\sigma_{v} v S\frac{\partial^{2}}{\partial S\,\partial v}
  + \frac{1}{2}\sigma_{v}^{2} v\frac{\partial^{2}}{\partial v^{2}}
  + \kappa(\theta-v)\frac{\partial}{\partial v}		
  + r S\frac{\partial}{\partial S},
\end{equation}
and rewrite the PDE as
\begin{equation}\label{eq:heston_pde_redux}
  \frac{\partial V}{\partial t} + \mathcal{L}^{\mathrm{Heston}}V - rV = 0.
\end{equation}

\subsubsection{Stochastic Interest Rates}

For maturities beyond a few months or for product where the underlying is a interest rate, modelling the short rate as a
constant parameter is inadequate \cite{brigo2013interest} as it fails to capture the
time-varying nature of interest rates. A more realistic approach is to assume that the
short rate \(r_t\) follows a stochastic process, which allows it to evolve in response
to macroeconomic factors, monetary policy, and other market dynamics.
There is a plethora of models that capture this behaviour, but two of the most popular are the
\emph{Vasicek} model \cite{VASICEK1977177} and the
\emph{Hull-White} model \cite{hullwhitemodel}.  
Both models assume that the short rate follows a mean-reverting process, which is
a common feature in interest rate dynamics.  The key difference is that the Hull-White model
allows for a time-dependent mean reversion level, making it more flexible in fitting the
current yield curve.

We can define both models in terms of the following stochastic differential equations (SDEs):
\begin{equation}\label{eq:vasicek_sde}
  \mathrm{d}r_t
  = \kappa(\theta_t-r_t)\,\mathrm{d}t
    + \sigma_r\,\mathrm{d}W^{\mathbb{Q},r}_t.
\end{equation}
where in both \(\kappa\) is the mean-reversion speed and \(\theta_t\) is the time-dependent long-run mean. 
In the Vasicek model, \(\theta_t\) is a constant, while in the Hull-White model it can be a deterministic 
function of time. A derivative whose value depends on the short rate , $V(t,r)$, has a pricing PDE
\begin{equation}\label{eq:vasicek_pde}
\begin{aligned}
  &\frac{\partial V}{\partial t}
  + \kappa(\theta_t-r)\,\frac{\partial V}{\partial r}
  + \frac12\sigma_r^{2}\,\frac{\partial^{2} V}{\partial r^{2}}
  - rV
  = 0.
\end{aligned}
\end{equation}
This PDE is derived in the same fashion as the ones from the previous sections. In operator form, we can write
\begin{equation}\label{eq:L_SR}
  \mathcal{L}^{\mathrm{SR}}
  := \frac12\sigma_r^{2}\frac{\partial}{\partial r^{2}}
  + \kappa(\theta_t-r)\frac{\partial}{\partial r},
\end{equation}
and rewrite the PDE as
\begin{equation}\label{eq:vasicek_pde_redux}
  \frac{\partial V}{\partial t} + \mathcal{L}^{\mathrm{SR}}V - rV = 0.
\end{equation}
What sets aside short-rate models from the previous sections is that they are \emph{affine} in the state variable \(r_t\).
Affine models are those whose dynamics can be expressed as a linear combination of the state variable and its
exponential function. This property allows for closed-form solutions for the prices of certain derivatives, 
such as zero-coupon bonds. See anex~\ref{sec:affine_models} for a more detailed discussion on affine models.


%---------------------------------------------------------------------
\subsection{Joint Stochastic Volatility and Stochastic Rates}\label{sec:sv_rates}

Finally, we drop both the assumption of constant volatility and the
assumption of constant interest rates, and consider a model that combines
stochastic volatility and stochastic interest rates. As already mentioned, this is desirable for
pricing long-dated products.



\section{Traditional Numerical Approaches}

\subsubsection{Brief Overview of Finite Difference Methods}
Finite Difference Methods (FDM) are among the most widely used techniques for numerically 
solving partial differential equations such as \eqref{eq:BS_single} or its multi-dimensional 
counterpart \eqref{eq:BS_multi}. The core idea behind FDM is to discretize the time and 
asset domains into a structured grid and approximate the derivatives in the PDE using finite 
differences.

For example, in the single-asset case, the time derivative can be approximated by a backward 
difference:
\begin{equation}
	\frac{\partial V}{\partial t} \approx \frac{V^{n} - V^{n-1}}{\Delta t},
\end{equation}
and spatial derivatives using central differences:
\begin{equation}
	\frac{\partial V}{\partial S} \approx \frac{V_{i+1}^n - V_{i-1}^n}{2\Delta S}
	\qquad
	\frac{\partial^2 V}{\partial S^2} \approx \frac{V_{i+1}^n - 2V_i^n + V_{i-1}^n}{\Delta S^2}
\end{equation}
This transforms the PDE into a system of algebraic equations, which can be solved using 
standard linear algebra techniques such as LU decomposition or iterative solvers.

Despite their simplicity and interpretability, FDMs suffer from the 
CoD. In the $d$-dimensional case, the number of grid points grows 
exponentially with $d$, making them impractical for high-dimensional problems. As a result, 
their application is limited to cases with small $d$ or where symmetry or decomposition 
strategies can be employed to reduce complexity.

\subsubsection{Monte Carlo Methods}
Monte Carlo (MC) methods are stochastic techniques that simulate multiple sample paths 
of the underlying assets under the risk-neutral measure $\mathbb{Q}$ and average the 
discounted payoff \cite{glasserman2004monte}. In the context of the multi-asset Black-Scholes 
model \eqref{eq:gbm_multi}, this involves simulating correlated geometric Brownian motions. 
An example of a MC pricing algorithm is shown below:
\begin{algorithm}[H]
	\caption{MC Pricing of a General Payoff}
	\begin{algorithmic}[1]
	\STATE \textbf{Input:} Number of simulations $N$, time horizon $T$, risk-free rate $r$, strike $K$, asset parameters $(\mu_i, \sigma_i, \rho_{ij})$
	\FOR{$n = 1$ to $N$}
	  \STATE Simulate one path for each asset $S_T^i$ using correlated Brownian motion
	  \STATE Compute the path-dependent payoff: $\text{Payoff}^{(n)} = \max\left(\max_i S_T^{i} - K, 0\right)$
	\ENDFOR
	\STATE Compute the option price:
	\[
	V(0, \mathbf{S}_0) \approx e^{-rT} \cdot \frac{1}{N} \sum_{n=1}^{N} \text{Payoff}^{(n)}
	\]
	\end{algorithmic}
\end{algorithm}
MC methods are particularly attractive for high-dimensional problems, 
as their convergence rate is independent of the number of dimensions. However, 
they converge slowly — with a rate of $\mathcal{O}(1/\sqrt{N})$ — so often require 
variance reduction techniques (e.g., control variates, antithetic variables) to be 
efficient.

Another drawback, where PDE methods outperform MC methods, is that they are not well-suited for
pricing American options or other path-dependent derivatives, as they require
the ability to exercise at multiple points in time. While there are techniques to adapt MC 
methods for American options, such as the Longstaff-Schwartz method \cite{longstaffshawrtz}, they are often less
efficient than PDE-based methods. This is where PINNs can provide a significant advantage, 
as they can naturally handle path-dependent payoffs and American-style options, among others.

\chapter{Physics-Informed Machine Learning}

\section{Introduction to Physics-Informed Neural Networks (PINNs)}
As previously discussed, traditional numerical methods for solving PDEs in 
derivative pricing, such as FDM and MC simulations, 
often struggle with computational complexity, particularly when dealing with 
high-dimensional financial instruments. PINNs, introduced by Raissi et al. \cite{RAISSI2019686}, 
provide an innovative framework that explicitly incorporates governing PDEs directly into neural 
network training. Rather than relying solely on large amounts of data, PINNs leverage physical 
laws and constraints embedded within the problem formulation to enhance predictive accuracy, 
efficiency, and reliability of the solutions. This methodology is especially promising for 
high-dimensional derivative pricing problems, as it effectively mitigates the 
CoD while maintaining consistency with the underlying financial theory.

Consider the PDE problem defined by:
\begin{equation}
	\begin{aligned}
	\mathcal{N}_I[u(t,x)] &= 0, && x\in\Omega,\quad t\in [0,T], \\
	\mathcal{N}_B[u(t,x)] &= 0, && x\in\partial\Omega,\quad t\in [0,T], \\
	\mathcal{N}_0[u(t^*,x)] &= 0, && x\in\Omega,\quad t^*=0,
	\end{aligned}
	\label{eq:PDE_conditions}
\end{equation}
where \(\mathcal{N}_I\) denotes the PDE operator in the interior of the spatial 
domain \(\Omega\), \(\mathcal{N}_B\) represents the boundary condition operator 
on \(\partial\Omega\), and \(\mathcal{N}_0\) enforces initial conditions at \(t=0\).

PINNs approximate the PDE solution \(u(t,x)\) with a neural network \(u_\theta(t,x)\), 
parameterized by \(\theta\). The optimal parameters \(\theta\) are found by minimizing 
the composite loss function:
\begin{equation}
	\mathcal{L}(\theta) = \lambda_1 \mathcal{L}_{\text{PDE}}(\theta) + \lambda_2 \mathcal{L}_{\text{boundary}}(\theta) + \lambda_3 \mathcal{L}_{\text{initial}}(\theta),
	\label{eq:total_loss}
\end{equation}
with each component explicitly defined as:
\begin{equation}
	\begin{aligned}
		\mathcal{L}_{\text{PDE}}(\theta) &= \int_{0}^{T}\int_{\Omega}\left|\mathcal{N}_I[u_\theta(t,x)]\right|^2\,\mathrm{d}x\,\mathrm{d}t, \\
		\mathcal{L}_{\text{boundary}}(\theta) &= \int_{0}^{T}\int_{\partial\Omega}\left|\mathcal{N}_B[u_\theta(t,x)]\right|^2\,\mathrm{d}s\,\mathrm{d}t, \\
		\mathcal{L}_{\text{initial}}(\theta) &= \int_{\Omega}\left|\mathcal{N}_0[u_\theta(0,x)]\right|^2\,\mathrm{d}x.
	\end{aligned}
	\label{eq:loss_terms}
\end{equation}
These integrals are typically approximated by Monte Carlo sampling, 
choosing discrete collocation points within the respective domains.

\section{Challenges in Training PINNs}
Training PINNs involves addressing several critical challenges:
\begin{itemize}
    \item \textbf{Loss imbalance:} Significant differences in the magnitude of PDE and boundary 
	losses can lead to convergence issues, addressed through adaptive weighting strategies \cite{wang2020understandingmitigatinggradientpathologies}.
    
    \item \textbf{Spectral bias:} Neural networks prioritize low-frequency solutions, complicating 
	representation of high-frequency or multi-scale phenomena~\cite{wang2023expertsguidetrainingphysicsinformed}.
    
    \item \textbf{Causality violations:} Training PINNs over entire temporal domains simultaneously 
	may violate physical causality, prompting the use of time-adaptive training schemes~\cite{wang2022respectingcausalityneedtraining}.
    
    \item \textbf{Computational complexity:} Extensive computational resources are needed, particularly 
	for high-dimensional PDEs, due to expensive gradient computations \cite{karniadakis2021physics}.
\end{itemize}

\section{Developments and Applications}
PINN's field of investigation has witnessed substantial advancements aimed at addressing 
their inherent limitations, such as training inefficiencies, scalability challenges, 
and difficulties in capturing complex solution behaviors. This section delineates notable 
developments that enhance the capability and applicability of PINNs.

\subsubsection{Extended Physics-Informed Neural Networks}
Extended Physics-Informed Neural Networks (XPINNs) introduce a generalized space-time domain 
decomposition strategy, partitioning the problem domain into multiple subdomains. 
Each subdomain is assigned a distinct neural network, facilitating parallel training 
and improved scalability. This approach is particularly effective for complex geometries 
and high-dimensional problems, as it allows for localized learning and reduces computational 
overhead~\cite{Hu_2022}.

\subsubsection{Variational and Augmented PINNs}
Variational PINNs (VPINNs) reformulate the loss function using variational principles, 
integrating test functions and quadrature rules to enhance solution accuracy and convergence 
rates. This method is beneficial for problems where traditional PINNs struggle with stability 
and precision~\cite{kharazmi2019variationalphysicsinformedneuralnetworks}. \textbf{Revisar} Augmented PINNs 
(APINNs) incorporate additional physical constraints or auxiliary variables into the neural network architecture, improving the network's 
ability to capture complex solution features and ensuring adherence to 
underlying physical laws~\cite{Hu_2023}.

\subsubsection{Neural Operators: DeepONet and Fourier Neural Operators}
Neural operators, such as Deep Operator Networks (DeepONet) and Fourier Neural 
Operators (FNO), represent a paradigm shift by learning mappings between function 
spaces rather than individual function evaluations. DeepONet employs a branch and 
trunk network to approximate nonlinear operators, enabling rapid predictions 
across varying input functions~\cite{Lu_2021}. FNO leverages the Fourier 
transform to learn global solution operators efficiently, offering advantages in 
handling complex, high-dimensional PDEs with reduced computational costs~\cite{li2021fourierneuraloperatorparametric}.

\subsubsection{Applications of PINNs}
In terms of applications, PINNs have been successfully employed in various fields, including:
\begin{itemize}
    \item \textbf{Fluid dynamics:} Modeling complex flows including turbulent 
	and laminar regimes, and inferring hidden physical fields from sparse observational 
	data \cite{RAISSI2019686, Bhatnagar_2019}.
    
    \item \textbf{Biomedicine:} Improving medical imaging reconstruction and personalized patient 
	modeling via inverse problems involving PDE constraints \cite{banerjee2024pinnsmedicalimageanalysis}.
    
    \item \textbf{Materials science and climate modeling:} Inferring material properties and 
	assimilating observational data into physics-based climate models for improved 
	predictions \cite{karniadakis2021physics}.
\end{itemize}
Overall, the PINN methodology represents a robust integration of deep learning and physical 
modeling, actively evolving through continuous theoretical improvements and expanding applications.

\subsubsection{Training PINNs}
As already said, training PINNs involves minimizing the total loss $\mathcal{L}(\theta)$ with respect to the
neural network parameters $\theta$. This is typically done using gradient-based optimization algorithms,
such as stochastic gradient descent (SGD) or its variants, which iteratively update the parameters
based on the computed gradients of the loss function with respect to the parameters. The gradients are
computed using automatic differentiation, which allows for efficient and accurate calculation of the gradients
of the loss function with respect to the parameters. The optimization process continues until a stopping criterion is met,
such as a maximum number of iterations or convergence of the loss function.

As usually data of the true solution is not available, the training process relies on sampling collocation points
from the domain of interest, which are used to compute the loss terms. The collocation points are synthetically generated
using a predefined procedure, for example, by sampling from a uniform distribution over the domain of interest.

A typical training procedure for PINNs is as follows:

\begin{algorithm}[H]
	\caption{Training procedure of PINNs}
	\begin{algorithmic}[1]
	\STATE \textbf{Input:} Neural network architecture, PDE operator, boundary and initial conditions.
	\STATE Initialize neural network parameters $\theta$.
	\STATE Sample total collocation points from $\Omega$, $\partial\Omega$ and $\Omega_0$.
	\WHILE{not converged}
	  \STATE Obtain a subset of collocation points from the total set.
	  \STATE Compute PDE residuals at subset of collocation points.
	  \STATE Compute total loss $\mathcal{L}(\theta)$ using \eqref{eq:total_loss} at the collocation points.
	  \STATE Compute gradients of $\mathcal{L}(\theta)$ with respect to $\theta$.
	  \STATE Update parameters $\theta$ using an optimization algorithm.
	\ENDWHILE
	\end{algorithmic}
	\label{alg:training_pinns}
\end{algorithm}

Given the multitude of options and techniques available for training PINNs, it is crucial to
carefully select the most appropriate methods for each specific problem. In the following sections,
we will discuss the various techniques and strategies that can be employed to enhance the training process
and improve the performance of PINNs. We will explore different neural network architectures,
sampling strategies, loss balancing techniques, and optimization methods that can be used to
train PINNs effectively. By understanding the strengths and weaknesses of each approach, we can
make informed decisions about which techniques to apply in our specific use case, ultimately leading to
more accurate and efficient solutions for pricing multi-asset derivatives.


% \chapter{Implementation and Experiments}

% In this chapter, we present the implementation details and benchmarks different techniques used in this thesis.
% We will discuss the architecture altenatives of the neural networks, the sampling strategies that can be employed,
% the optimization techniques used to train the models, and the various experiments conducted to evaluate 
% the best approach for pricing multi-asset derivatives.

% \section{Sampling Point Strategies}

% \section{Neural Network Arquitectures}

% When we talk about the architecture of a neural network, we are referring to the number of 
% layers and the number of neurons in each layer, as well as the activation functions used or 
% any other modification we want to apply to the networks.

% \subsubsection{Feedforward Neural Network}

% The first arquitectures and most traditional arquitecture is the feedforward neural network. It is a
% fully connected neural network, where each neuron in a layer is connected to every neuron in the
% previous layer. The output of each neuron is computed as a weighted sum of the inputs, followed by
% an activation function. Some of the most common activation functions are the sigmoid, hyperbolic tangent (tanh),
% and rectified linear unit (ReLU) functions and their choice can have a significant impact on the performance of the network.

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width=0.5\textwidth]{imagenes/ffnn.png}
% 	\caption{Feedforward neural network: a fully connected neural network,
% 		where each neuron in a layer is connected to every neuron in the previous layer.}
% 	\label{fig:ffnn}
% \end{figure}

% \subsubsection{Neural Network with Ansatz Layer}

% The second arquitecture is inspired in \cite{SUKUMAR2022114333}, where Sukumar et al. propose modifying the
% traditional feedforward neural network by adding an ansatz layer in order to impose exact 
% satisfaction of the boundary conditions. This method in turn simplifies the training process,
% as the neural network is only required to satisfy the PDE and boundary conditions, rather than the
% initial condition. In other words, the ansatz layer is a function that takes the output of the neural 
% network and passes it as an argument of an ansatz function $f(u)$ that attempts to provide an 
% approximation of the real solution of the problem. In our case, the form of the ansatz function is
% \begin{equation}
% 	\begin{aligned}
% 	\text{Payoff}(\mathbf{x}) &= \max\left(0, \max_{1 \leq j \leq d} \left( e^{x_j} - 1 \right) \right), \\
% 	f(u_\theta, \mathbf{x}, \tau) &= \text{Payoff}(\mathbf{x}) + u_\theta(\mathbf{x}, \tau) \times \tau
% 	\end{aligned}
% 	\end{equation}
	


% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width=1\textwidth]{imagenes/ansatz.png}
% 	\caption{Neural network with ansatz layer: the main change in this setup is the addition of an ansatz function $f(u)$
% 		that takes the output of the neural network and passes it as an argument.}
% 	\label{fig:ffnn_ansatz}
% \end{figure}

% \subsubsection{First-Order Neural Network}

% Finally, the last approach we explore attempts to enhance the training process by complementing the output
% of the neural network by also providing estimates of the first order derivatives of the solution. This
% requires us adding a new loss term to the total loss function, which is computed against
% the derivates using automatic differentiation (compatibility loss.)

% The befenits of this approach are twofold: first, it allows us to
% compute the derivatives of the solution without having to compute them using automatic differentiation,
% which can be computationally expensive. Second, it allows us to use the derivatives of the solution
% as additional information to guide the training process, which can help improve convergence and accuracy.

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width=0.8\textwidth]{imagenes/ansatz.png}
% 	\caption{Neural network with ansatz layer.}
% 	\label{fig:ansatz_layer}
% \end{figure}

% \subsubsection{Performance Metrics and Benchmarks}

% Now we proceed to evaluate the performance of the different neural network architectures.
% For this tests, we analyze how each arquitecture behaves under the following option
% parameters:

% % table
% \begin{table}[H]
% 	\caption{Option parameters.}
% 	\label{tab:params}
% 	\centering
% 	\begin{tabular}{|c|c|c|c|c|}
% 		\hline
% 		\textbf{Parameter} & \textbf{Value} \\
% 		\hline
% 		Number of assets $d$ & 1 \\
% 		\hline
% 		Strike $K$ & \$15 \\
% 		\hline
% 		Maturity $T$ & 1.0 \\
% 		\hline
% 		Risk-free rate $r$ & 0.05 \\
% 		\hline
% 		Diffusion $\sigma_i$ & 0.2 \\
% 		\hline
% 		Corrleation $\rho_{ij}$ & 0.25 \\
% 		\hline
% 	\end{tabular}
% \end{table}

% First, we begin by analysing the $L_2$ relative error evolution of the different
% arquitectures during the training process (Figure \ref{fig:rel_error}). The $L_2$ relative error is defined as
% \begin{equation}
% 	\text{Rel. Error} = \frac{\|u_\theta - u^*\|_2}{\|u^*\|_2},
% \end{equation}
% where $u_\theta$ is the output of the neural network and $u^*$ is the real solution. In this thesis,
% two different metrics are used to compute the $L_2$ relative error: when time evolutions of the error are presented,
% this are computed using the MC solution because it is the only possible solution when multiple assets are involved.
% When the error is computed across the surface, the $L_2$ relative error is computed using the analytical solution
% of the problem, which is available in the single asset case.

% The first thing we
% can notice is that all networks are able to produce solutions with relative small 
% errors, but the FFNN and the FOPINN outperform the FFNN with ansatz layer in the long run. Although the FFNN with ansatz layer
% is capable of producing a solution with small relative error after a few training iterations,
% is unable to progress further and the error stagnates. This could be related to serval issues: first, 
% the ansatz solution is not differentiable, making the computation of the gradients
% difficult. This could lead to a situation where the gradients are not informative enough to guide the training process. 
% Second, the fact the optimization process is not having information about the initial condition seems to restrict 
% the ability of the network to learn the solution. 

% This can be seen in Figure \ref{fig:nets_heatmaps}, where the FFNN with ansatz layer
% produces higher errors close to the strike at the inital condition, diffusing the error to the rest of the domain.
% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width=1\textwidth]{imagenes/nets_heatmaps.png}
% 	\caption{$L_2$ errors accros the surface.}
% 	\label{fig:nets_heatmaps}
% \end{figure}
% Another interesting thing to notice is that the FFNN and the FOPINN are both able to produce solutions with relative errors
% of around $10^{-4}$, but the FFNN is able to do it in less steps. This can be explained by the additional compatibility
% loss term that the FOPINN has to compute, which makes the training process harder. As a benefit, the FOPINN is able to produce
% not only the solution of the problem, but also the first order derivatives of the solution, which can be useful in applications where the
% \textit{greeks} \cite{alma99148840908702021} are required.
% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width=1\textwidth]{imagenes/nets_error_histories.png}
% 	\caption{$L_2$ relative error evolution of the different architectures during the training process.}
% 	\label{fig:rel_error}
% \end{figure}

% \section{Loss Balancing Strategies}
% Loss balacing is another studied technique in PINNs, which aims to 
% improve also the convergence and accuracy of the solution by assigning specific weights to the 
% different loss terms in the total loss function. According to \cite{wang2023expertsguidetrainingphysicsinformed},
% gradient pathologies... 

% \subsection{GradNorm}

% \subsection{ReBRoLaBRo}

% \subsection{Performance Metrics and Benchmarks}

% \subsection{Optimization techniques}

% \subsubsection{Gradient Descent}
% \subsubsection{Quasi-Newton Methods}
% \subsubsection{Performance Metrics and Benchmarks}

\chapter{Implementations and Results}
\section{Implementation Details}
\section{Call Option}
We begin by attempting to price a call option using the Black-Scholes model. The price surface can be 
seen in Figure \ref{fig:call_option}.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{imagenes/heston_hullwhite_call.png}
	\caption{Call option pricing using the Black-Scholes model. The neural network 
	is able to produce a solution with a relative error of around $10^{-4}$.}
	\label{fig:call_option}
\end{figure}

The relative error of the solution is shown in Figure \ref{fig:call_option_error}, where we can see that the error is concentrated around the maturity, close to the strike price,
which is expected as the payoff of a call option is not differentiable near this region.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{imagenes/vanilla_put_error.png}
	\caption{Relative error of the call option pricing using the Black-Scholes model. The error is concentrated around the maturity, close to the strike price.}
	\label{fig:call_option_error}
\end{figure}

This results were obtained using a small feedforward neural network with just one hidden layer with 10 neurons, 
and a total of 1000 collocation points sampled uniformly across the domain. The training process took around 1000 iterations to converge, and the total loss was computed using the PDE residuals and the boundary conditions.
collocation points, drawn from a uniform distribution across the domain. Training took around 1000 iterations to converge, 
with the total time of 2 minutes in total. 

\subsection{Multi-Asset Option}

As metioned in previous sections, we can extend the previous case to many assets in order to assess PINNs
capabilities to tackle PDEs with multiple state variables. We train again a small neural networks but now
using the dimensionless form of the Black-Scholes PDE defined in \eqref{eq:BS_multi}. Figure \ref{fig:multi_asset_call} shows the price surface 
for 2, 3, 5, and 7 assets.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{imagenes/nets_surfaces.png}
	\caption{Call option pricing using the Black-Scholes model for multiple assets. The neural network is able to produce a solution with a relative error of around $10^{-4}$.}
	\label{fig:multi_asset_call}
\end{figure}

As expected, the problems becomes more challenging as the number of assets (dimensions) increases. To asses this increased complexity, we train each
model for the same number of iterations and number of collocation points. 

\subsection{Call Option with Stochastic Volatility and Interest Rates}

Finally, we can extend the previous case to include stochastic volatility and interest rates. In 
Figure \ref{fig:call_option_heston_hullwhite}, we shown the behaviour of the price surface for different values of the
volatility of volatility. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{imagenes/heston_hullwhite_surfaces.png}
	\caption{Call option pricing using the Heston-Hull-White model. The neural network is able to produce a solution with a relative error of around $10^{-4}$.}
	\label{fig:call_option_heston_hullwhite_surface}
\end{figure}

As expected, higher values make the surface flatter as the 
uncertainty of the underlying increases, as seen in Figure \ref{fig:call_option_heston_hullwhite_vols}. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{imagenes/heston_hullwhite_vols.png}
	\caption{Call option pricing using the Heston-Hull-White model. The neural network is able to produce a solution with a relative error of around $10^{-4}$.}
	\label{fig:call_option_heston_hullwhite}
\end{figure}

Another key feature as already mentioned is that for longer maturities, interest rate curves have a significant impact on the
price of the option, making this feature a key component of the model. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{imagenes/heston_hullwhite_rates.png}
	\caption{Call option pricing using the Heston-Hull-White model. The neural network is able to produce a solution with a relative error of around $10^{-4}$.}
	\label{fig:call_option_heston_hullwhite_interest_rates}
\end{figure}

\section{Put Option}
The next product is the European Put Option, which is similar to the call option but with a different payoff function. 
The price surface can be seen in Figure \ref{fig:put_option}. Notice that in this case, the stock axis is reversed, as the payoff of a put option is the opposite of a call option.
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{imagenes/vanilla_put_surface.png}
	\caption{European put prices. At the left, the price surface using a PINN and at the right, the price surface using the exact solution.}
	\label{fig:put_option}
\end{figure}

The absolute error of the solution is shown in Figure \ref{fig:put_option_error}, 
where we can see that the error is concentrated around the maturity, 
close to the strike price, which is expected as the payoff of 
a put option is not differentiable near this region.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{imagenes/vanilla_put_error.png}
	\caption{Absolute error of the put option pricing using the Black-Scholes model. The error is concentrated around the maturity, close to the strike price.}
	\label{fig:put_option_error}
\end{figure}

\subsection{American Put Option}

A interesting variant of the put option is the American Put Option, which allows the holder to exercise the option at any time before maturity.
This variantion makes the problem more complex, as we need to take into account the early exercise feature of the option.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{imagenes/american_put_surface.png}
	\caption{American put prices. At the left, the price surface using a PINN and at the right, the price surface using the exact solution.}
	\label{fig:american_put_option}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{imagenes/american_put_error.png}
	\caption{Absolute error of the American put option pricing using the Black-Scholes model. The error is concentrated around the maturity, close to the strike price.}
	\label{fig:american_put_option_error}
\end{figure}

\section{Swaption}

Finally, our last product to evaluate is the European Swaption, for which we use the Hull-White model to price it.

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{imagenes/hullwhite_swaption_surface.png}
	\caption{European swaption prices. At the left, the price surface using a PINN and at the right, the price surface using the exact solution.}
	\label{fig:swaption_surface}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{imagenes/hullwhite_swaption_error.png}
	\caption{European swaption prices. At the left, the price surface using a PINN and at the right, the price surface using the exact solution.}
	\label{fig:swaption_error}
\end{figure}

\chapter{Conclusions and Future Work}
\section{Discussion}
\section{Summary of Findings}
\section{Limitations and Future Research Directions}


\clearpage
\addcontentsline{toc}{chapter}{Bibliography}

\printbibliography

\end{document}